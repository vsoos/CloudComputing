{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Lz5RpFj2P_9NJUcDlvlgm2d41PeDwJNj",
      "authorship_tag": "ABX9TyNJnznx0UUm8pZ1J5VR/Ax8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsoos/CloudComputing/blob/main/ex3/3_notebook3_CPU/3_3_cpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining models\n",
        "**Feedback tone + Sarcasm detection**"
      ],
      "metadata": {
        "id": "EkCyAI0LD4EQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.9.1 transformers==4.20.1"
      ],
      "metadata": {
        "id": "BVd-rdN5HnS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis"
      ],
      "metadata": {
        "id": "9NoshW6OSuls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm # Progress Bar\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import transformers\n",
        "from transformers import DistilBertTokenizer, TFDistilBertModel, DistilBertConfig\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "import warnings\n",
        "from transformers import logging as hf_logging\n",
        "hf_logging.set_verbosity_error() # Hidding Huggingface Warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "DsCF_VkTHz9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_labels = ['Negative', 'Neutral', 'Positive']\n",
        "sarcasm_labels = ['Normal', 'Sarcastic']"
      ],
      "metadata": {
        "id": "Yc5BtZKcIhJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# paths to model files (h5), weights\n",
        "sentiment_model_file = \"/content/drive/MyDrive/cloudcomputing2023_VincenzinaSoos/ex3/3_notebook1_GPU/data/sentiment_model.h5\"\n",
        "sarcasm_model_file = \"/content/drive/MyDrive/cloudcomputing2023_VincenzinaSoos/ex3/3_notebook2_GPU/sarcasm_model.h5\""
      ],
      "metadata": {
        "id": "FkAuz5-sIf2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 235\n",
        "MODEL_NAME = 'distilbert-base-cased'"
      ],
      "metadata": {
        "id": "fA0v4CpIIQTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = DistilBertConfig.from_pretrained(MODEL_NAME, output_hidden_states=True, output_attentions=True, return_dict=True)\n",
        "DistilBERT = TFDistilBertModel.from_pretrained(MODEL_NAME, config=config)\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(MAX_LENGTH,), name='input_token', dtype='int32')\n",
        "attention_mask_in = tf.keras.layers.Input(shape=(MAX_LENGTH,), name='masked_token', dtype='int32')\n",
        "\n",
        "embedding_layer = DistilBERT(input_ids=input_ids_in, attention_mask=attention_mask_in)[0]\n",
        "X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(64, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(3, activation='softmax')(X)\n",
        "\n",
        "sentiment_model = tf.keras.Model(inputs=[input_ids_in, attention_mask_in], outputs=X)\n",
        "\n",
        "for layer in sentiment_model.layers[:3]:\n",
        "    layer.trainable = False\n",
        "\n",
        "sentiment_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5alCGPnHeis",
        "outputId": "291b5be1-5e78-4099-efed-1e017e8caa3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 235)]        0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 235)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model_1 (TFDist  TFBaseModelOutput(l  65190912   ['input_token[0][0]',            \n",
            " ilBertModel)                   ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 235, 768),                                                   \n",
            "                                 hidden_states=((No                                               \n",
            "                                ne, 235, 768),                                                    \n",
            "                                 (None, 235, 768),                                                \n",
            "                                 (None, 235, 768),                                                \n",
            "                                 (None, 235, 768),                                                \n",
            "                                 (None, 235, 768),                                                \n",
            "                                 (None, 235, 768),                                                \n",
            "                                 (None, 235, 768)),                                               \n",
            "                                 attentions=((None,                                               \n",
            "                                 12, None, 235),                                                  \n",
            "                                 (None, 12, None, 2                                               \n",
            "                                35),                                                              \n",
            "                                 (None, 12, None, 2                                               \n",
            "                                35),                                                              \n",
            "                                 (None, 12, None, 2                                               \n",
            "                                35),                                                              \n",
            "                                 (None, 12, None, 2                                               \n",
            "                                35),                                                              \n",
            "                                 (None, 12, None, 2                                               \n",
            "                                35)))                                                             \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 235, 256)     918528      ['tf_distil_bert_model_1[0][13]']\n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 256)         0           ['bidirectional[0][0]']          \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 64)           16448       ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 64)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            195         ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 66,126,083\n",
            "Trainable params: 935,171\n",
            "Non-trainable params: 65,190,912\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME,\n",
        "                                                add_special_tokens=True,\n",
        "                                                max_length=MAX_LENGTH,\n",
        "                                                pad_to_max_length=True)\n",
        "\n",
        "def tokenize(sentences, tokenizer):\n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "    for sentence in tqdm(sentences):\n",
        "        inputs = tokenizer.encode_plus(sentence,\n",
        "                                       add_special_tokens=True,\n",
        "                                       max_length=MAX_LENGTH,\n",
        "                                       pad_to_max_length=True,\n",
        "                                       return_attention_mask=True,\n",
        "                                       return_token_type_ids=True,\n",
        "                                       truncation=True)\n",
        "        input_ids.append(inputs['input_ids'])\n",
        "        input_masks.append(inputs['attention_mask'])\n",
        "        input_segments.append(inputs['token_type_ids'])\n",
        "\n",
        "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32')"
      ],
      "metadata": {
        "id": "lByRxX9nHegW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the weights from the h5 -file trained previously\n",
        "sentiment_model.load_weights(sentiment_model_file)"
      ],
      "metadata": {
        "id": "CSc0AUdnHebN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sarcasm detection model"
      ],
      "metadata": {
        "id": "EEqewugaJLrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mispell_dict = {\"ain't\": \"is not\", \"cannot\": \"can not\", \"aren't\": \"are not\", \"can't\": \"can not\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",\n",
        "                \"doesn't\": \"does not\",\n",
        "                \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "                \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\",\n",
        "                \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n",
        "                \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "                \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
        "                \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\", \"this's\": \"this is\", \"that'd\": \"that would\",\n",
        "                \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\", \"they'd\": \"they would\",\n",
        "                \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
        "                \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
        "                \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\",\n",
        "                \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"wont\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
        "                \"wouldn't\": \"would not\",\n",
        "                \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n",
        "                \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color',\n",
        "                'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n",
        "                'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What',\n",
        "                'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I',\n",
        "                'theBest': 'the best', 'howdoes': 'how does', 'Etherium': 'Ethereum',\n",
        "                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what',\n",
        "                'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
        "\n",
        "mispell_dict = {k.lower(): v.lower() for k, v in mispell_dict.items()}"
      ],
      "metadata": {
        "id": "otI2LqD4JOAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_text(s):\n",
        "    # making our string lowercase & removing extra spaces\n",
        "    s = str(s).lower().strip()\n",
        "\n",
        "    # remove contractions.\n",
        "    s = \" \".join([mispell_dict[word] if word in mispell_dict.keys() else word for word in s.split()])\n",
        "\n",
        "    # removing \\n\n",
        "    s = re.sub('\\n', '', s)\n",
        "\n",
        "    # put spaces before & after punctuations to make words seprate. Like \"king?\" to \"king\", \"?\".\n",
        "    s = re.sub(r\"([?!,+=—&%\\'\\\";:¿।।।|\\(\\){}\\[\\]//])\", r\" \\1 \", s)\n",
        "\n",
        "    # Remove more than 2 continues spaces with 1 space.\n",
        "    s = re.sub('[ ]{2,}', ' ', s).strip()\n",
        "\n",
        "    return s"
      ],
      "metadata": {
        "id": "nEFQLfQwJP9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sarcasm_data = pd.read_csv(\"/content/drive/MyDrive/cloudcomputing2023_VincenzinaSoos/ex3/3_notebook2_GPU/data/train-balanced-sarcasm.csv\")\n",
        "\n",
        "sarcasm_data.drop(['author', 'subreddit', 'score', 'ups', 'downs', 'date', 'created_utc', 'parent_comment'], axis=1, inplace=True)\n",
        "# remove empty rows\n",
        "sarcasm_data.dropna(inplace=True)\n",
        "sarcasm_data.head()\n",
        "\n",
        "# let's take a small piece for testing purposes for error metrics\n",
        "# ~~ 10% would be 100000 rows.\n",
        "sarcasm_data = sarcasm_data.iloc[0:950000]\n",
        "sarcasm_data['comment'] = sarcasm_data['comment'].apply(preprocessing_text)"
      ],
      "metadata": {
        "id": "n2XatjTGJRds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# total unique words we are going to use.\n",
        "TOTAL_WORDS = 40000\n",
        "\n",
        "# max number of words one sentence can have\n",
        "MAX_LEN = 50\n",
        "\n",
        "# width of of 1D embedding vector\n",
        "EMBEDDING_SIZE = 300"
      ],
      "metadata": {
        "id": "e3uR15dzJSot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keras for deep learning model creation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model"
      ],
      "metadata": {
        "id": "4HN7nGc1JdRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Keras version in TensorFlow:\", tf.keras.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfFpYNeDKrgN",
        "outputId": "b955ff37-d4e8-4913-aff2-46f6efa16028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.9.1\n",
            "Keras version in TensorFlow: 2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tokenizer2 = Tokenizer(num_words=TOTAL_WORDS)\n",
        "tokenizer2.fit_on_texts(list(sarcasm_data['comment']))\n",
        "\n",
        "train_data = tokenizer2.texts_to_sequences(sarcasm_data['comment'])\n",
        "train_data = pad_sequences(train_data, maxlen = MAX_LEN)\n",
        "target = sarcasm_data['label']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f8-yfcCJn_L",
        "outputId": "81feef03-c99f-4f31-9f7e-d5b8dcb87be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 36.6 s, sys: 228 ms, total: 36.8 s\n",
            "Wall time: 37.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "EMBEDDING_FILE = \"/content/drive/MyDrive/cloudcomputing2023_VincenzinaSoos/ex3/3_notebook2_GPU/crawl-300d-2M.vec\"\n",
        "\n",
        "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in tqdm(open(EMBEDDING_FILE)))\n",
        "\n",
        "# remember ot use tokenizer2\n",
        "word_index = tokenizer2.word_index\n",
        "nb_words = min(TOTAL_WORDS, len(word_index))\n",
        "embedding_matrix = np.zeros((nb_words, EMBEDDING_SIZE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIdL6nIfJpbL",
        "outputId": "aa220084-edc7-4876-9446-f6763744edd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1999996it [04:13, 7875.02it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 55s, sys: 9.12 s, total: 3min 4s\n",
            "Wall time: 4min 13s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filter out a smaller portion of the word embeddings\n",
        "for word, i in tqdm(word_index.items()):\n",
        "    if i >= TOTAL_WORDS: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3dQd7IJMPzR",
        "outputId": "8eafba7f-2553-4aa5-a99e-42c2ee31c350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 161878/161878 [00:00<00:00, 306490.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_layer = Input(shape=(MAX_LEN,))\n",
        "\n",
        "embedding_layer = Embedding(TOTAL_WORDS, EMBEDDING_SIZE, weights = [embedding_matrix])(input_layer)\n",
        "\n",
        "LSTM_layer = Bidirectional(LSTM(128, return_sequences = True))(embedding_layer)\n",
        "maxpool_layer = GlobalMaxPool1D()(LSTM_layer)\n",
        "\n",
        "dense_layer_1 = Dense(64, activation=\"relu\")(maxpool_layer)\n",
        "dropout_1 = Dropout(0.5)(dense_layer_1)\n",
        "\n",
        "dense_layer_2 = Dense(32, activation=\"relu\")(dropout_1)\n",
        "dropout_2 = Dropout(0.5)(dense_layer_2)\n",
        "\n",
        "output_layer = Dense(1, activation=\"sigmoid\")(dropout_2)\n",
        "\n",
        "# the original code uses the old way of Tensorflow\n",
        "# for input and output layer, just remove the keywords\n",
        "# input= and output= and it should work\n",
        "sarcasm_model = Model(input_layer, output_layer)\n",
        "\n",
        "sarcasm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "sarcasm_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ_svcfJMR-y",
        "outputId": "dba73d5f-7c53-45c1-bf5b-ee528742c0c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 50)]              0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 50, 300)           12000000  \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 50, 256)          439296    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 256)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                16448     \n",
            "                                                                 \n",
            " dropout_39 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_40 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,457,857\n",
            "Trainable params: 12,457,857\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sarcasm_model.load_weights(sarcasm_model_file)"
      ],
      "metadata": {
        "id": "GwxEvpnxNTUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the combined model"
      ],
      "metadata": {
        "id": "WbIRSydXMv--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert predictions to decimal format for easier reading (instead of scientific format)\n",
        "np.set_printoptions(formatter={'float_kind':'{:f}'.format})"
      ],
      "metadata": {
        "id": "LdMNRB-CMzUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# our test data\n",
        "test_list = [\"Excellent breakfast and room service. Highly recommended!\",\n",
        "             \"Rude staff and the rooms were not very clean. Also very expensive. Never again!\",\n",
        "             \"Okay hotel for a good price, nothing special.\",\n",
        "             \"I just totally love it when the rooms are dirty and the AC is not working.\",\n",
        "             \"Totally worth the price! Said no one ever. UGH\"]\n",
        "\n",
        "# process the data for the first model\n",
        "test_values_sentiment_model = tokenize(test_list, tokenizer)\n",
        "test_probs = sentiment_model.predict(test_values_sentiment_model)\n",
        "counter = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQwIh1RZM2Bh",
        "outputId": "357f4e6d-a436-4f1a-9ab0-235d8c140a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 1057.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for test_prob in test_probs:\n",
        "  print(test_list[counter])\n",
        "  print(test_prob)\n",
        "  tone_index = np.argmax(test_prob)\n",
        "  print(sentiment_labels[tone_index])\n",
        "\n",
        "  # get the original sentence and preprocess it for the MODEL 2\n",
        "  sentence = preprocessing_text(test_list[counter])\n",
        "  sentence = tokenizer2.texts_to_sequences([sentence])\n",
        "  sentence = pad_sequences(sentence, maxlen = MAX_LEN)\n",
        "\n",
        "  # get sarcasm % from MODEL 2\n",
        "  sarcasm_prediction = sarcasm_model.predict(sentence)\n",
        "  sarcasm_prediction_value = sarcasm_prediction[0][0]\n",
        "\n",
        "  print(f\"Sarcasm%: {round(sarcasm_prediction_value * 100, 1)} %\")\n",
        "\n",
        "  print(\"---------------------------------\")\n",
        "  counter += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0Cz6H-OO3aB",
        "outputId": "592724ac-36ac-4644-a637-b795ebe3c5f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Excellent breakfast and room service. Highly recommended!\n",
            "[0.000013 0.000489 0.999498]\n",
            "Positive\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Sarcasm%: 38.9 %\n",
            "---------------------------------\n",
            "Rude staff and the rooms were not very clean. Also very expensive. Never again!\n",
            "[0.836917 0.117928 0.045154]\n",
            "Negative\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Sarcasm%: 53.6 %\n",
            "---------------------------------\n",
            "Okay hotel for a good price, nothing special.\n",
            "[0.490344 0.359617 0.150039]\n",
            "Negative\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Sarcasm%: 25.0 %\n",
            "---------------------------------\n",
            "I just totally love it when the rooms are dirty and the AC is not working.\n",
            "[0.873319 0.092828 0.033853]\n",
            "Negative\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "Sarcasm%: 77.7 %\n",
            "---------------------------------\n",
            "Totally worth the price! Said no one ever. UGH\n",
            "[0.053020 0.021234 0.925746]\n",
            "Positive\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Sarcasm%: 99.3 %\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and evaluate each feedback\n",
        "results = []\n",
        "for feedback in test_list:\n",
        "    # sentiment model\n",
        "    processed_feedback = tokenize([feedback], tokenizer)\n",
        "    sentiment_probs = sentiment_model.predict(processed_feedback)\n",
        "    sentiment_index = np.argmax(sentiment_probs[0])\n",
        "    sentiment = sentiment_labels[sentiment_index]\n",
        "\n",
        "    # sarcasm model\n",
        "    preprocessed_feedback = preprocessing_text(feedback)\n",
        "    preprocessed_feedback = tokenizer2.texts_to_sequences([preprocessed_feedback])\n",
        "    preprocessed_feedback = pad_sequences(preprocessed_feedback, maxlen=MAX_LEN)\n",
        "    sarcasm_prob = sarcasm_model.predict(preprocessed_feedback)[0][0]\n",
        "    sarcasm = \"Yes\" if sarcasm_prob > 0.5 else \"No\"\n",
        "\n",
        "    # Evaluate based on provided logic\n",
        "    outcome = \"Undefined outcome\"\n",
        "    if sentiment == \"Positive\":\n",
        "        outcome = \"Negative feedback\" if sarcasm == \"Yes\" else \"Positive feedback\"\n",
        "    elif sentiment == \"Neutral\":\n",
        "        outcome = \"Neutral feedback\"\n",
        "    elif sentiment == \"Negative\":\n",
        "        outcome = \"Positive feedback?\" if sarcasm == \"Yes\" else \"Negative feedback\"\n",
        "\n",
        "    results.append((feedback, sentiment, sarcasm, round(sarcasm_prob * 100, 1), outcome))\n",
        "\n",
        "# Print results\n",
        "for feedback, tone, sarcasm, sarcasm_percent, outcome in results:\n",
        "    print(f\"Feedback: {feedback}\\nSentiment: {sentiment}\\nSarcasm: {sarcasm} ({sarcasm_percent}%)\\nOutcome: {outcome}\\n---------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh3_wQx8O5CY",
        "outputId": "0eb388a4-aa23-49aa-a475-c3e9b228abcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 680.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 986.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 488ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1205.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 455ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1582.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 676ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1153.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Feedback: Excellent breakfast and room service. Highly recommended!\n",
            "Sentiment: Positive\n",
            "Sarcasm: No (38.9%)\n",
            "Outcome: Positive feedback\n",
            "---------------------------------\n",
            "Feedback: Rude staff and the rooms were not very clean. Also very expensive. Never again!\n",
            "Sentiment: Positive\n",
            "Sarcasm: Yes (53.6%)\n",
            "Outcome: Positive feedback?\n",
            "---------------------------------\n",
            "Feedback: Okay hotel for a good price, nothing special.\n",
            "Sentiment: Positive\n",
            "Sarcasm: No (25.0%)\n",
            "Outcome: Negative feedback\n",
            "---------------------------------\n",
            "Feedback: I just totally love it when the rooms are dirty and the AC is not working.\n",
            "Sentiment: Positive\n",
            "Sarcasm: Yes (77.7%)\n",
            "Outcome: Positive feedback?\n",
            "---------------------------------\n",
            "Feedback: Totally worth the price! Said no one ever. UGH\n",
            "Sentiment: Positive\n",
            "Sarcasm: Yes (99.3%)\n",
            "Outcome: Negative feedback\n",
            "---------------------------------\n"
          ]
        }
      ]
    }
  ]
}